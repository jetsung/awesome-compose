import{_ as s,o as a,c as e,aj as t}from"./chunks/framework.DpVzuYkC.js";const F=JSON.parse('{"title":"open-webui","description":"","frontmatter":{},"headers":[],"relativePath":"open-webui/index.md","filePath":"open-webui/README.md"}'),n={name:"open-webui/index.md"};function h(l,i,p,k,r,o){return a(),e("div",null,[...i[0]||(i[0]=[t('<h1 id="open-webui" tabindex="-1">open-webui <a class="header-anchor" href="#open-webui" aria-label="Permalink to “open-webui”">​</a></h1><p><a href="https://openwebui.com/" target="_blank" rel="noreferrer">Office Web</a> - <a href="https://github.com/open-webui/open-webui" target="_blank" rel="noreferrer">Source</a> - <a href="https://github.com/orgs/open-webui/packages/container/package/open-webui" target="_blank" rel="noreferrer">Docker Image</a> - <a href="https://docs.openwebui.com/" target="_blank" rel="noreferrer">Document</a></p><hr><blockquote><p><a href="https://openwebui.com/" target="_blank" rel="noreferrer">open-webui</a> 是一个可扩展 、功能丰富且用户友好的自托管 AI 平台，旨在完全离线运行。 它支持各种 LLM 运行器，如 Ollama 和 OpenAI 兼容的 API，并内置了 RAG 推理引擎 ，使其成为强大的 AI 部署解决方案 。</p></blockquote><hr><h3 id="installation-with-default-configuration" tabindex="-1">Installation with Default Configuration <a class="header-anchor" href="#installation-with-default-configuration" aria-label="Permalink to “Installation with Default Configuration”">​</a></h3><ul><li><p><strong>If Ollama is on your computer</strong>, use this command:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --add-host=host.docker.internal:host-gateway</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:main</span></span></code></pre></div></li><li><p><strong>If Ollama is on a Different Server</strong>, use this command:</p><p>To connect to Ollama on another server, change the <code>OLLAMA_BASE_URL</code> to the server&#39;s URL:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -e</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> OLLAMA_BASE_URL=https://example.com</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:main</span></span></code></pre></div></li><li><p><strong>To run Open WebUI with Nvidia GPU support</strong>, use this command:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpus</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> all</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --add-host=host.docker.internal:host-gateway</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:cuda</span></span></code></pre></div></li></ul><h3 id="installation-for-openai-api-usage-only" tabindex="-1">Installation for OpenAI API Usage Only <a class="header-anchor" href="#installation-for-openai-api-usage-only" aria-label="Permalink to “Installation for OpenAI API Usage Only”">​</a></h3><ul><li><p><strong>If you&#39;re only using OpenAI API</strong>, use this command:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -e</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> OPENAI_API_KEY=your_secret_key</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:main</span></span></code></pre></div></li></ul><h3 id="installing-open-webui-with-bundled-ollama-support" tabindex="-1">Installing Open WebUI with Bundled Ollama Support <a class="header-anchor" href="#installing-open-webui-with-bundled-ollama-support" aria-label="Permalink to “Installing Open WebUI with Bundled Ollama Support”">​</a></h3><p>This installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:</p><ul><li><p><strong>With GPU Support</strong>: Utilize GPU resources by running the following command:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpus=all</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ollama:/root/.ollama</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:ollama</span></span></code></pre></div></li><li><p><strong>For CPU Only</strong>: If you&#39;re not using a GPU, use this command instead:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ollama:/root/.ollama</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:ollama</span></span></code></pre></div></li></ul>',12)])])}const g=s(n,[["render",h]]);export{F as __pageData,g as default};
